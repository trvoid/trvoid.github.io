[
{
	"uri": "/_header/",
	"title": "",
	"tags": [],
	"description": "",
	"content": "trvoid\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": " trvoid 기술 문서 작성을 위한 소프트웨어 개발자의 웹사이트\n문서 작성 원칙  한글 사용자를 위한 문서를 작성합니다. 이론과 실무를 균형있게 다룹니다. 타인의 지식을 존중합니다.  "
},
{
	"uri": "/web/web01-hugo-docdock-github/",
	"title": "Hugo + DocDock 웹사이트를 GitHub Pages로 호스팅하기",
	"tags": [],
	"description": "",
	"content": " Hugo 프레임워크에 DocDock 테마를 추가하여 정적 웹사이트를 만들고 이를 GitHub 저장소에 올려서 GitHub Pages로 호스팅하는 과정을 정리하였습니다.\n1 제품 소개 1.1 Hugo  정적 웹사이트를 생성하는 도구. Go 언어로 개발함.  1.2 DocDock  기술 문서 작성을 위한 Hugo용 테마. Learn 테마를 기반으로 함.  1.3 GitHub Pages  정적 웹사이트 호스팅 서비스를 무료로 제공. GitHub 저장소와 직접 연결. 개인, 조직, 프로젝트 유형에 따른 페이지 제공. 사이트 저장 용량은 최대 1GB.  2 Hugo와 DocDock 설치 다음과 같은 환경에서 설치를 진행하고 이 문서를 작성하였습니다.\n 프로세서: Intel Core i5 (x64 기반) 운영체제: Windows 10 (64 비트)  2.1 Hugo 설치 1) Hugo Releases 페이지에서 다음 파일을 다운로드하고 압축을 풉니다.\nhugo_0.25.1_Windows-64bit.zip  2) 압축을 푼 폴더를 환경 변수 PATH에 추가하고 사이트를 생성하고자 하는 폴더에서 명령창을 엽니다.\n3) 다음과 같이 명령을 실행하여 새 Hugo 사이트를 생성합니다.\nC:\\DevApps\\hugo_0.25.1_Windows-64bit\u0026gt;hugo new site quickstart Congratulations! Your new Hugo site is created in C:\\DevApps\\hugo_0.25.1_Windows-64bit\\quickstart. Just a few more steps and you're ready to go: 1. Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/, or create your own with the \u0026quot;hugo new theme \u0026lt;THEMENAME\u0026gt;\u0026quot; command. 2. Perhaps you want to add some content. You can add single files with \u0026quot;hugo new \u0026lt;SECTIONNAME\u0026gt;\\\u0026lt;FILENAME\u0026gt;.\u0026lt;FORMAT\u0026gt;\u0026quot;. 3. Start the built-in live server via \u0026quot;hugo server\u0026quot;. Visit https://gohugo.io/ for quickstart guide and full documentation.  위 명령은 quickstart 폴더에 새 사이트를 만듭니다.\n4) 위에서 생성한 Hugo 사이트 폴더를 Git 저장소로 만듭니다.\n\u0026gt; cd quickstart \u0026gt; git init  이후부터 본문에서 언급하는 폴더들과 명령창에서 실행하는 명령들의 기준 위치는 quickstart 폴더입니다.\n2.2 테마 DocDock 추가 1) Hugo 테마 DocDock을 themes/dockdock 폴더에 추가합니다.\n\u0026gt; git submodule add https://github.com/vjeantet/hugo-theme-docdock.git themes/docdock  2) 테마 DocDock을 사용하도록 config.toml 파일에 다음 한 줄을 추가합니다.\ntheme = \u0026quot;docdock\u0026quot;  3 컨텐츠 생성 컨텐츠는 content 폴더 아래에서 Markdown 문법을 사용하여 작성합니다. 폴더의 계층 구조는 웹사이트 좌측 메뉴바의 계층 구조로 나타납니다. 또한 각 폴더의 _index.md 파일에서 작성한 내용은 해당 폴더의 주 화면에 표시됩니다.\n3.1 컨텐츠 파일 작성 실습을 위하여 content 폴더 아래에 다음 파일들을 만들고 제시한 내용들을 저장합니다.\n _header.md\n화면 좌측 상단에 표시할 문구를 작성합니다.\nQuickStart  _index.md\n홈 화면에서 표시할 문서를 작성합니다.\n--- Title: \u0026quot;Home\u0026quot; --- # Home  ble/_index.md\nble 폴더의 주 화면에서 표시할 문서를 작성합니다.\n--- Title: \u0026quot;Bluetooth Low Energy\u0026quot; ---  ble/ble01-intro.md\nble 폴더의 하위 메뉴로 표시할 문서를 작성합니다.\n--- Title: \u0026quot;BLE01 - Introduction\u0026quot; ---  ble/ble02-physical-layer.md\nble 폴더의 하위 메뉴로 표시할 문서를 작성합니다.\n--- Title: \u0026quot;BLE02 - Physical layer\u0026quot; ---  ml/_index.md\nml 폴더의 주 화면에서 표시할 문서를 작성합니다.\n--- Title: \u0026quot;Machine Learning\u0026quot; ---  ml/ml01-intro.md\nml 폴더의 하위 메뉴로 표시할 문서를 작성합니다.\n--- Title: \u0026quot;ML01 - Introduction\u0026quot; ---  ml/ml02-linear-regression.md\nml 폴더의 하위 메뉴로 표시할 문서를 작성합니다.\n--- Title: \u0026quot;ML02 - Linear regression\u0026quot; ---   3.2 개발 서버로 웹사이트 검증 1) Hugo 개발 서버를 실행합니다.\n\u0026gt; hugo server  2) 브라우져로 아래 주소에 연결합니다.\nhttp://localhost:1313  3) 화면 왼쪽 사이드바에 메뉴가 나타나고 각각의 메뉴 항목을 클릭하여 위에서 작성한 모든 컨텐츠를 볼 수 있는지 확인합니다.\n4 컨텐츠 배치 content 폴더 아래에서 작성한 컨텐츠를 GitHub Pages 서비스를 통해서 호스팅하려면 먼저 배치용 컨텐츠로 변환해야 합니다. 변환 결과는 public 폴더 아래에 저장되고 public 폴더 아래의 내용을 GitHub 저장소에 올리면 바로 웹으로 서비스됩니다. GitHub 계정의 사용자 이름이 yourname 이라고 하면 GitHub 저장소 주소와 여기에 연결된 웹사이트 주소는 다음과 같습니다.\nGitHub 저장소 주소:\nhttps://github.com/yourname/yourname.github.io.git  GitHub Pages 웹사이트 주소:\nhttps://yourname.github.io  4.1 설정 config.toml 파일을 열고 baseURL 항목의 값을 /로 지정합니다.\nbaseURL = \u0026quot;/\u0026quot; languageCode = \u0026quot;en-us\u0026quot; title = \u0026quot;QuickStart\u0026quot; theme = \u0026quot;docdock\u0026quot;  4.2 GitHub 저장소를 서브모듈로 추가 GitHub의 웹사이트 저장소를 서브모듈로 public 폴더 아래에 추가합니다.\n\u0026gt; git submodule add https://github.com/yourname/yourname.github.io.git public  4.3 컨텐츠를 배치용으로 변환 아래와 같이 명령을 실행하여 배치용 컨텐츠로 변환합니다.\n\u0026gt; hugo  위 명령이 끝나면 public 폴더에서 배치용 컨텐츠를 찾을 수 있습니다.\n4.4 컨텐츠를 GitHub에 배치 1) public 폴더의 변경 사항을 commit하고 push하면 GitHub 저장소에 반영됩니다.\n2) 브라우져를 열고 다음 주소에 연결합니다\nhttps://yourname.github.io  3) 화면 왼쪽 사이드바에 메뉴가 나타나고 각각의 메뉴 항목을 클릭하여 위에서 작성한 모든 컨텐츠를 볼 수 있는지 확인합니다.\n참고 자료  Hugo Quick Start Hugo Themes How To Install and Use Hugo, a Static Site Generator, on Ubuntu 14.04 What is GitHub Pages? Host on GitHub  "
},
{
	"uri": "/ml/ml02-linear-regression/",
	"title": "ML02 - 선형 회귀",
	"tags": [],
	"description": "",
	"content": " 1 소개 회귀 분석 과정을 다음 세 단계로 간단하게 정리할 수 있습니다.\n 훈련 데이터 세트 (x, y)를 준비합니다. 여기서 x는 독립 변수, y는 종속 변수입니다. 훈련 데이터 세트를 표현할 수 있는 모델을 만들고 가장 근접한 결과를 보여 주는 파라미터를 찾습니다. 새로운 데이터의 x값이 주어질 때 앞에서 얻은 모델을 사용하여 y값을 예측합니다.  모델을 만들 때 종속 변수 y를 독립 변수 x의 일차식으로 표현하면 이를 선형 회귀라고 말합니다. 그리고 x가 하나의 변수이면 일변량 선형 회귀, 둘 이상의 변수이면 다변량 선형 회귀라고 합니다.\n이 문서를 작성하면서 사용하는 주요 용어들은 다음과 같습니다.\n hypothesis - 모델을 나타내는 함수 식 feature - 독립 변수 x의 개별 요소 cost function - 훈련 데이터 세트의 종속 변수 y와 모델의 예측값의 차이를 나타내는 함수 식  2 일변량 선형 회귀 2.1 Hypothesis와 cost function 훈련 데이터의 feature가 한 개일 때 hypothesis는 다음과 같이 표현할 수 있습니다.\n$$ { h }_{ \\theta }(x)={ \\theta }_{ 0 }+{ \\theta }_{ 1 }{ x }_{ 1 } $$ 위의 식에서 \\(x\\)는 데이터의 feature들이고 \\(\\theta\\)는 찾고자 하는 파라미터들입니다. \\(x_0=1\\)라고 하면 위의 식을 다음과 같은 형태로 표현할 수 있습니다.\n$$ h_\\theta(x)=x^T\\cdot\\theta $$ $$ x=(x_0,x_1), \\theta=(\\theta_0,\\theta_1) $$ 데이터의 개수가 \\(m\\)일 때 cost function은 다음과 같이 표현할 수 있습니다. \\(y\\)는 데이터의 결과값이고 \\((i)\\)는 \\(i\\)번째 데이터임을 의미합니다.\n$$ J(\\theta )=\\frac { 1 }{ 2m } \\sum _{ i=1 }^{ m }{ { \\left( { h }_{ \\theta }({ x }^{ (i) })-{ y }^{ (i) } \\right) }^{ 2 } } $$ hypothesis를 대입하여 위 식을 전개하면 cost function은 각각의 파라미터에 대하여 아래로 볼록한 2차 함수가 됩니다.\n$$ J(\\theta )=\\frac { 1 }{ 2m } \\sum _{ i=1 }^{ m }{ \\left( { \\theta }_{ 0 }^{ 2 }+{ \\theta }_{ 1 }^{ 2 }{ x }_{ 1 }^{ (i)2 }+{ y }_{ }^{ (i)2 }+2{ \\theta }_{ 0 }{ \\theta }_{ 1 }{ x }_{ 1 }^{ (i) }-2{ \\theta }_{ 1 }{ x }_{ 1 }^{ (i) }{ y }_{ }^{ (i) }-2{ y }_{ }^{ (i) }{ \\theta }_{ 0 } \\right) } $$ 이 cost function이 최소값을 가지도록 하는 파라미터 \\(\\theta\\)를 찾는 방법은 3 다변량 선형 회귀 단원에서 다루겠습니다.\n3 다변량 선형 회귀 3.1 Hypothesis와 cost function 훈련 데이터의 feature가 \\(n\\)개일 때 hypothesis는 다음과 같이 표현할 수 있습니다. \\(x\\)는 데이터의 feature들이고 \\(\\theta\\)는 찾고자 하는 파라미터들입니다.\n$$ { h }_{ \\theta }(x)={ \\theta }_{ 0 }+{ \\theta }_{ 1 }{ x }_{ 1 }+{ \\theta }_{ 2 }{ x }_{ 2 }+\\cdots +{ \\theta }_{ n }{ x }_{ n } $$ 데이터의 개수가 \\(m\\)일 때 cost function은 다음과 같이 표현할 수 있습니다. \\(y\\)는 데이터의 결과값이고 \\((i)\\)는 \\(i\\)번째 데이터임을 의미합니다.\n$$ J(\\theta )=\\frac { 1 }{ 2m } \\sum _{ i=1 }^{ m }{ { \\left( { h }_{ \\theta }({ x }^{ (i) })-{ y }^{ (i) } \\right) }^{ 2 } } $$ 3.2 Gradient descent cost function의 편미분을 사용하여 cost를 최소화하는 파라미터 $\\theta$를 찾을 수 있습니다. 다음은 gradient descent 방식을 나타내는 알고리즘입니다.\n$$ repeat \\quad \\{ \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad\\\\ { \\theta }_{ j }:={ \\theta }_{ j }-\\alpha \\frac { \\partial J(\\theta ) }{ \\partial { \\theta }_{ j } } \\\\ \\} \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad $$ $$ \\frac { \\partial J(\\theta ) }{ \\partial { \\theta }_{ j } } =\\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ \\left( { h }_{ \\theta }({ x }^{ (i) })-{ y }^{ (i) } \\right) { { x }_{ j } }^{ (i) } } $$ 위의 식에서 $\\alpha$는 학습률(learning rate)입니다.\n3.3 방정식의 해 아래로 볼록한 함수의 경우 기울기가 $0$일 때 최소값을 가지므로 다음 방정식을 풀어서 cost를 최소가 되게 하는 $\\theta$를 구할 수도 있습니다.\n$$ \\frac { \\partial J(\\theta ) }{ \\partial { \\theta }_{ j } } =\\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ \\left( { h }_{ \\theta }({ x }^{ (i) })-{ y }^{ (i) } \\right) { { x }_{ j } }^{ (i) } } = 0 $$ 위 방정식을 $\\theta$에 대해서 풀면 아래의 결과를 얻습니다.\n$$ \\theta ={ \\left( { X }^{ T }\\cdot X \\right) }^{ -1 }\\cdot { X }^{ T }\\cdot y $$ 여기서 $X$와 $y$는 다음과 같습니다.\n$$ X=\\begin{bmatrix} { x }_{ 0 }^{ (1) } \u0026 { x }_{ 1 }^{ (1) } \u0026 { x }_{ 2 }^{ (1) } \u0026 \\cdots \\\\ { x }_{ 0 }^{ (2) } \u0026 { x }_{ 1 }^{ (2) } \u0026 { x }_{ 2 }^{ (2) } \u0026 \\cdots \\\\ { x }_{ 0 }^{ (3) } \u0026 { x }_{ 1 }^{ (3) } \u0026 { x }_{ 2 }^{ (3) } \u0026 \\cdots \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \\end{bmatrix}, \\quad y=\\begin{bmatrix} { y }^{ (1) } \\\\ { y }^{ (2) } \\\\ { y }^{ (3) } \\\\ \\vdots \\end{bmatrix} $$ 하지만 이 방식은 몇 가지 단점을 가지고 있습니다.\n 행렬 ${ { X }^{ T }\\cdot X }$에 대한 역행렬이 존재하지 않을 수도 있습니다. $n$이 커짐에 따라 계산 비용이 gradient descent 방식보다 빠른 속도로 증가합니다.  "
},
{
	"uri": "/ml/ml03-logistic-regression/",
	"title": "ML03 - 로지스틱 회귀",
	"tags": [],
	"description": "",
	"content": " 1 문제를 정의하기 1.1 데이터 세트 이해를 돕기 위하여 다음과 같이 두 종류의 데이터 세트를 준비하고 결과를 비교해 봅니다.\nDataset-A:\n$$x=\\left[ 1,2,3,4,5,6,7,8,9,10 \\right] ,\\quad y=[0,0,0,0,0,1,1,1,1,1]$$\nDataset-B:\n$$x=\\left[ 1,2,3,4,5,6,7,8,9,10 \\right] ,\\quad y=[0,0,0,0,1,0,1,1,1,1]$$\n위에서 x는 독립 변수이고 y는 종속 변수입니다. y는 0 또는 1을 값으로 가집니다.\n1.2 단계별로 과제 해결 아래에서 제시하는 단계별로 과제를 해결합니다.\n 데이터 세트를 잘 나타내는 hypothesis를 찾습니다. 데이터 세트의 y값과 hypothesis의 예측값의 차이를 나타내는 cost function을 정의합니다. 각각의 데이터 세트에 대하여 cost를 최소화하는 hypothesis의 파라미터를 찾습니다.  이렇게 얻은 hypothesis를 사용하여 새로운 데이터의 y값을 예측합니다.\n2 Hypothesis 찾기 다음은 hypothesis를 정의하는데 도움이 될만한 데이터 세트의 특성입니다.\n 데이터의 x값에 따라 y값이 0과 1로 구분됩니다. x값이 어떤 임계값보다 작을 때는 y값이 0, 크면 y값이 1이라고 말할 수 있습니다. y값이 0인 데이터와 1인 데이터의 x값이 서로 겹치는 범위가 데이터 세트에 따라 매우 좁을 수도 있고 넓을 수도 있습니다.  이러한 특성을 고려하여 다음과 같은 의미를 가지는 hypothesis를 찾고자 합니다.\n$$p(y=1|x;w)$$\n위 표현은 파라미터 w가 정해지고 x값이 주어질 때 y값이 1일 가능성을 의미합니다. p가 가지는 값의 범위는 $0 \\sim 1$입니다. p의 값이 0.5 이상이면 y값이 1, 0.5 미만이면 y값이 0이라고 판정합니다.\n2.1 첫번째 시도 다음과 같이 hypothesis는 x의 1차식이라고 가정해 봅니다.\n$${ h }_{ w }(x)=w_0+w_1x$$\n그러면 $h_w(x)$가 가질 수 있는 값의 범위는 $-\\infty \\sim +\\infty $가 되어 찾고자 하는 hypothesis가 될 수 없습니다.\n2.2 두번째 시도 x의 일차식은 단조증가하거나 단조감소합니다. 따라서 좌변은 이러한 조건을 만족시키는 $h_w(x)$의 함수식이어야 합니다. y=0일 가능성 대비 y=1일 가능성의 비율이 단조증가하므로 hypothesis를 다음과 같이 가정해 봅니다.\n$$\\frac {h_w(x)}{1 - h_w(x)}=w_0+w_1x$$\n하지만 $h_w(x)$가 $0 \\sim 1$ 사이의 값을 가질 때 좌변이 가질 수 있는 값의 범위는 $0 \\sim +\\infty$로 여전히 우변이 가질 수 있는 값의 범위와 일치하지 않습니다.\n참고로 y=0일 가능성 대비 y=1일 가능성의 비율을 odds ratio라고 부릅니다.\n$$odds(p) = \\frac {p} {1-p} =\\frac {성공할 \\quad 가능성} {실패할 \\quad 가능성}$$\n2.3 세번째 시도 값의 범위가 $0 \\sim +\\infty$일 때 로그를 적용하면 단조증가하면서 값의 범위가 $-\\infty \\sim +\\infty$로 확장됩니다. 그래서 이번에는 좌변에 로그를 적용해 봅니다.\n$$log(\\frac {h_w(x)}{1 - h_w(x)})=w_0+w_1x$$\nOdds ratio에 로그를 적용한 것을 로짓(logit) 함수라고 부릅니다.\n이제 좌변과 우변 모두 동일한 값의 범위를 가지도록 하는 방법을 찾았습니다. 위 식을 hypothesis에 대하여 풀면 다음과 같습니다.\n$$h_w(x)=\\frac {1} {1+{ e }^{ -(w_0 + w_1x) }}$$\n$x_0=1$이라고 정의하면 위 수식은 다음과 같이 표현할 수 있습니다.\n$$w=(w_0,w_1), \\quad x=(x_0,x_1)$$ $$w_0x_0 + w_1x_1 = w^Tx = z(x)$$ $$h_w(x)=\\frac {1} {1+{ e }^{ -z(x) }}$$\n이러한 형태의 함수를 로지스틱(logistic) 함수라고 합니다. 또한 그래프로 그려 보면 S자 모양과 비슷하며 시그모이드(sigmoid) 함수라고 부르기도 합니다.\n여기서 눈여겨 볼 것은 좌변에서 사용할 $h_w(x)$의 함수식을 찾을 때 다음 두 가지를 만족시킬 수 있다면 logit 함수가 아니어도 된다는 사실입니다.\n 단조증가 또는 단조감소 값의 범위가 $-\\infty \\sim +\\infty $  문제: logit 함수가 아닌 $h_w(x)$의 함수식을 찾아 보시오. 3 비용 함수 정의하기 선형 회귀에서와 같이 최소자승법을 사용하여 비용 함수를 정의하면 non-convex 함수가 되어 최소값을 찾는 것이 어려워집니다.\n$$J(w)=\\frac { 1 }{ m } \\sum _{ i=1 }^{ m } \\frac {1} {2} { (h_w(x^{(i)}) - y^{(i)})^2 }$$\n그래서 convex 함수가 되도록 비용 함수를 정의해 보고자 합니다.\n$$J(w)=\\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ Cost(h_w(x^{(i)}), y^{(i)}) }$$\n3.1 첫번째 시도 개별 데이터에 대한 비용을 계산할 때 사용할 수식을 아래와 같이 정의하면 데이터 세트에 대한 비용 함수는 convex 형태가 됩니다.\n$$Cost(h_{ w }(x^{ (i) }),y^{ (i) })=\\quad \\quad h_w(x^{(i)}), \\quad for \\quad y^{(i)}=0 $$\n$$Cost(h_{ w }(x^{ (i) }),y^{ (i) })=1 - h_w(x^{(i)}), \\quad for \\quad y^{(i)}=1 $$\n이것을 하나의 식으로 표현하면 다음과 같습니다.\n$$Cost(h_{ w }(x^{ (i) }),y^{ (i) })=(1-y^{(i)}) h_w(x^{(i)}) + y^{(i)}(1-h_w(x^{(i)})$$\n위의 정의에 따르면 y값이 0일 때 예측값이 1이 될 가능성이 0에 가까워지면 비용이 줄어들고, 1에 가까워지면 비용이 늘어나게 됩니다. 마찬가지로 y값이 1일 때 예측값이 1이 될 가능성이 0에 가까워지면 비용이 늘어나고 1에 가까워지면 비용이 줄어듭니다. 따라서 비용 함수를 위와 같이 정의하는 것이 타당하다고 할 수 있습니다.\n3.2 두번째 시도 그런데 log를 사용하면 수학적으로 더 편리할뿐만 아니라 gradient descent 방식으로 최소값을 찾을 때 더 빠른 속도로 최소값에 접근하게 됩니다. 그래서 log를 사용해서 비용 함수를 다시 정의하면 다음과 같습니다.\n$$Cost(h_{ w }(x^{ (i) }),y^{ (i) }) = -log(1-h_w (x^{(i)})), \\quad {for} \\quad y^{(i)} = 0$$\n$$Cost(h_{ w }(x^{ (i) }),y^{ (i) }) = \\quad \\quad -log(h_w (x^{(i)})), \\quad {for} \\quad y^{(i)} = 1$$\n이것을 하나의 식으로 표현하면 다음과 같습니다.\n$$Cost(h_{ w }(x^{ (i) }),y^{ (i) })=-y^{(i)} log(h_w(x^{(i)})) - (1-y^{(i)})log(1-h_w(x^{(i)})$$\n첫번째 시도에서 얻은 비용 함수와 비교해 보면 y값이 0일 때 예측값이 1이 될 가능성이 1에 가까워지면 비용이 훨씬 더 가파르게 증가합니다. gradient descent 방식에서는 경사가 심할수록 더 빠른 속도로 최소값에 접근합니다.\n이제 하나의 데이터 세트에 대한 비용 함수를 다음과 같이 표현할 수 있습니다.\n$$J(w)=\\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ \\left[ -y^{(i)} log(h_w(x^{(i)})) - (1-y^{(i)})log(1-h_w(x^{(i)}) \\right]}$$\n4 파라미터 찾기 4.1 Gradient descent 비용 함수의 편미분을 사용하여 비용을 최소화하는 파라미터 w를 찾을 수 있습니다. 다음은 gradient descent 방식을 나타내는 알고리즘입니다.\n$$ repeat \\quad \\{ \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad\\\\ { w }_{ j }:={ w }_{ j }-\\alpha \\frac { \\partial J(w) }{ \\partial { w }_{ j } } \\\\ \\} \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad $$ $$ \\frac { \\partial J(w) }{ \\partial { w }_{ j } } =\\frac { 1 }{ m } \\sum _{ i=1 }^{ m }{ \\left( { h }_{ w }({ x }^{ (i) })-{ y }^{ (i) } \\right) { { x }_{ j } }^{ (i) } } $$ 위의 식에서 $\\alpha$는 학습률(learning rate)입니다.\n4.2 파이썬 코드 아래 링크는 실습을 위해 작성한 파이썬 노트북입니다.\n ML03-로지스틱 회귀  이 문서의 1.1 데이터 세트에서 제시한 두 종류의 데이터 세트에 대하여 각각 로지스틱 회귀를 적용하여 hypothesis의 파라미터를 찾습니다.\n0, w: [ 1. 1.], total_cost: 3.26021492096 100, w: [-2.6355604 0.42721842], total_cost: 0.381607976117 200, w: [-4.20800179 0.33990582], total_cost: 0.259597699813 300, w: [-5.11107361 0.44791876], total_cost: 0.189260375224 400, w: [-5.67752132 0.55875972], total_cost: 0.14696429209 500, w: [-6.12954266 0.60842371], total_cost: 0.136285798335 600, w: [-6.52973682 0.64520282], total_cost: 0.128206376451 700, w: [-6.89004447 0.67842379], total_cost: 0.121658132782 800, w: [-7.21891096 0.70882576], total_cost: 0.116203273473 900, w: [-7.52228277 0.7369322 ], total_cost: 0.111561580443 999, w: [-7.80178475 0.76287476], total_cost: 0.107581904528 0, w: [ 1. 1.], total_cost: 1.26021492096 100, w: [-1.28554202 0.32823582], total_cost: 0.305950256446 200, w: [-2.03450316 0.42156667], total_cost: 0.285936892902 300, w: [-2.3862255 0.46758369], total_cost: 0.281598132252 400, w: [-2.57198045 0.49243347], total_cost: 0.280395854669 500, w: [-2.67574608 0.50646611], total_cost: 0.280021810942 600, w: [-2.73543813 0.51458526], total_cost: 0.279898223002 700, w: [-2.77033872 0.51934772], total_cost: 0.279856010509 800, w: [-2.79093454 0.52216342], total_cost: 0.279841317035 900, w: [-2.80315452 0.52383585], total_cost: 0.279836145885 999, w: [-2.81037234 0.52482432], total_cost: 0.279834324659  Gradient descent를 1000번 적용하여 얻은 w는 다음과 같습니다.\nDataset-A\n999, w: [-7.80178475 0.76287476], total_cost: 0.107581904528  Dataset-B\n999, w: [-2.81037234 0.52482432], total_cost: 0.279834324659  다음은 위에서 찾은 w를 사용하여 그래프로 그린 것입니다.\n5 참고 자료  The Sigmoid Function in Logistic Regression by Karl Rosaen What is a Logit Function and Why Use Logistic Regression? by KAREN GRACE-MARTIN  "
},
{
	"uri": "/ml/ml04-softmax-regression/",
	"title": "ML04 - 소프트맥스 회귀",
	"tags": [],
	"description": "",
	"content": " 소개 hypothesis 함수로 소프트맥스(softmax) 함수를 사용합니다. 이것은 다항 로지스틱 회귀(multinomial logistic regression)라고도 하는데 2개보다 많은 클래스로 분류하고자 할 때 사용할 수 있습니다.\n주요 용어  다항 로지스틱 회귀 (Multinomial logistic regression) 소프트맥스 함수 Cross entropy  다항 로지스틱 회귀 참고 문서  Multinomial Logistic Regression from Wikipedia Softmax Regression from UFLDL Tutorial What is one hot encoding and when is it used in data science? Log-linear model from Wikipedia  "
},
{
	"uri": "/mathjax-test/",
	"title": "MathJax 테스트",
	"tags": [],
	"description": "",
	"content": " $$ { h }_{ \\theta }(x)={ \\theta }_{ 0 }+{ \\theta }_{ 1 }{ x }_{ 1 } $$  "
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/ml/",
	"title": "기계 학습",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/web/",
	"title": "웹",
	"tags": [],
	"description": "",
	"content": ""
}]